

선형대수랑 maximum liklihood estimation 
트랜스포머 논문 절반정도 읽었음

### 1장
여러 프레임워크들
- Keras : wrapper, 간단
- Tenserflow : keras와 합쳐짐, Define and run (먼저 정의하고 실행 시점에 feed)
> production 측면에서 장점들이 있음
- pyTorch : low + high level 지원, Dynamic computation graph (실행하면서 그래프 생성)
> 디버깅 쉽고, 구현 측면에서 간단

pyTorch
- Numpy + AutoGrad + Function
- Numpy 함수들이 호환되는 Tensor 객체로 연산, 자동미분 및 관련 함수들, Multi-GPU

### 2장

Operations
- Tensor 객체는 np의 ndarray와 비슷.
- 지원 dtype도 동일, tensor는 GPU 지원 (.device 로 어느 계산장치에 있는지)
- 슬라이싱, flatten(), ones_like(), shape, dtype 등등 동일

- view : tensor의 shape을 변환해줌 -> view는 reference / reshape는 copy.
- squeeze : 차원의 개수가 1인 차원 제거 (어느 axis에 추가할지 param 지정 가능) 
- unsqueeze : 차원의 개수가 1인 차원 추가 (어느 axis에 추가할지 param 지정 가능)

- 사칙연산은 element-wise하게 작동 (shape이 맞아야 함)
- 행렬곱은 dot이 아니라 ==mm (matmul)을 사용== 
- matmul은 broadcasting을 지원함! (차원이 안 맞으면 알아서 채움)

nn.functional as F
- 기능적 함수들 몇가지 지원 like softmax(), argmax(), one_hot() 등

AutoGrad
- 자동미분
```python
w = torch.tensor(2.0)
y = w**2
z = 10*y + 25 # 따라서 z(w) = 10w^ + 25
z.backward()  # z를 미분
w.grad()      # w의 gradient를 구해줌
```
- ==복습하기!== 너무 난잡해보이는데, backward()의 내장적으로 어떤 일들이 일어나는지?

### 3장
여러 프로젝트 템플릿이 있음
[Star 4.3k](https://github.com/victoresque/pytorch-template)
[Star 1.2k](https://github.com/Lightning-AI/deep-learning-project-template)

```
pytorch-template/
│
├── train.py - main script to start training
├── test.py - evaluation of trained model
│
├── config.json - holds configuration for training
├── parse_config.py - class to handle config file and cli options
│
├── new_project.py - initialize new project with template files
│
├── base/ - abstract base classes
│   ├── base_data_loader.py
│   ├── base_model.py
│   └── base_trainer.py
│
├── data_loader/ - anything about data loading goes here
│   └── data_loaders.py
│
├── data/ - default directory for storing input data
│
├── model/ - models, losses, and metrics
│   ├── model.py
│   ├── metric.py
│   └── loss.py
│
├── saved/
│   ├── models/ - trained models are saved here
│   └── log/ - default logdir for tensorboard and logging output
│
├── trainer/ - trainers
│   └── trainer.py
│
├── logger/ - module for tensorboard visualization and logging
│   ├── visualization.py
│   ├── logger.py
│   └── logger_config.json
│  
└── utils/ - small utility functions
    ├── util.py
    └── ...
```

주석 달아보면서 동작 이해해보기.

### 4장
torch.nn.Module
- layer의 base class
- ==input, output, forward, backward 를 정의==해야 함
- 학습 대상이 되는 parameter (tensor) 정의해야 함

torch.nn.Parameter
- Tensor의 상속 객체
- nn.Module 내의 attr이 될 때는 required_grad = True가 되어서, Autograd의 대상이 됨.
- 직접 지정할 일은 없음 (layer에 이미 weight가 지정되어 있으므로)
```python
import torch.nn.Module

class Mylinear(nn.Module) :
	def __init__(self, in_feature, out_feature, bias=True)
		super().__init__()
		self.in_feature = in_feature
		self.out_feature = out_feature
		self.weights = nn.Parameter(torch.randn(in_feature, out_feature))
		self.bias = nn.Parameter(torch.randn(out_feature))

	def forward(self, x: Tensor) :
		return x @ self.weights + self.bias  # Wx + b

mymodel = Mylinear(7,2) # input node는 7개, output node는 2개.
``` 


backward()
- param들의 미분 수행. 기본 과정
```python
for epoch in range(epochs) :
	optimizer.zero_grad()   # gradient 초기화
	outputs = model(inputs) # pred_y 얻음
	loss = criterion(outputs, labels) # 기준에 따라 loss값 구함
	loss.backward() # 역전파
	optimizer.step() # 가중치 업데이트
```

- 오버라이딩은 가능하지만, Autograd가 직접 해줌

### 5장
전체적인 흐름
- Data (모으기, 전처리, 텐서 변환) -> Dataset (init, len, getitem 오버라이딩 필요) -> DataLoader


Dataset
- 데이터 입력 표준. Image, text, audio 등에 따라 
```python
import torch
from torch.utils.data import Dataset

class CustomDataset(Dataset) :
	def __init__(self, text, labels) : # 초기 데이터 생성 방법 지정
		self.labels = labels
		self.data = data

	def __len__(self) :
		return len(self.labels) # 데이터 전체 길이

	def __getitem__(self, idx) : # 부를때마다 text와 class 쌍을 반환.
		label = self.labels[idx]
		text = self.data[idx]
		return {"Text" : text, "Class" : label}
```

- 데이터 종류에 따라 init을 잘 정의해야 함.
- 데이터 생성 시점에 모든 걸 처리하기보다, 학습이 필요한 시점에서 변환해도 됨
- 데이터셋에 따른 표준안 필요한 상태...


Dataloader
- Dataset을 Dataloader로 투입할 때는 `batch_size`와 `shuffle` 등의 param을 설정 가능
- GPU feed 직전 데이터 변환을 책임. Tensor로 변환 밑 Batch 처리.

```python
MyDataLoader = DataLoader(MyDataset, batch_size = 2, shufffle= True)
for dataset in MyDataLoader : # 이런 식으로 iter 가능. batch size 크기만큼의 텐서 불러옴.
	print(dataset)
```

- `sampler`, `batch_sampler` : 샘플링 기법 지정
- `collate_fn` : padding할 때 정의


Torchvision의 Dataset에서 MNIST 코드 활용해보기.

### 6장
model.save()
- 모델 형태 및 파라미터를 저장
- epoch마다 저장하면서 최선의 결과 모델 선택하기
```python
torch.save(model, os.path.join(PATH, "model.pth")) # 구조와 함께 저장
model = torch.load(os.path.join(PATH, "model.pth")) # 구조와 함께 load


torch.save(model.state_dict(), os.path.join(PATH, "model.pth")) # 파라미터만 저장

new_model = ModelClass() # 모델은 별도로 구성
new_model.load_state_dict(torch.load(os.path.join(PATH, "model.pth"))) # 파라미터만 불러옴
```

checkpoint
- early stopping을 사용할 경우 이전 학습의 결과물을 저장하면서 가야 함. 
- loss와 metric을 함께 저장.
```python

# 저장하기
torch.save({
	'epoch' : e,
	'model_state_dict' : model.state_dict(),
	'optimizer_state_dict' : optimizer.state_dict(),
	'loss' : epoch_loss,
	},
	"ckpt_{}_{}.pt".format(e, epoch_loss)
	)

# 불러오기
ckpt = torch.load(PATH)
model.load_state_dict(ckpt['model_state_dict'])
optimizer.load_state_dict(ckpt['optimizer_state_dict'])
epoch = ckpt['epoch']
loss = ckpt['loss']

```

Transfer Learning (Fine-tuning)
- 다른 Dataset으로 만든 모델을 자기 데이터에 적용.
- pre-trained model은 기존 layer들은 freeze하고, 맨 뒤 몇 레이어만 backpropagation함.


### 7장
TensorBoard
- TF로 만들어진 도구, 그래프와 metric 등을 시각화.
- pytorch에도 연결 가능

- scalar : epoch마다 metric을 표시.
- histogram : weight의 분포 표현
- image : 예측과 실제 비교
- mesh : 3d형태 표현.

```python
from torch.utils.tensorboard import SummaryWriter # import
writer = SummaryWriter(PATH) # 객체 생성

for epoch in range(epochs) :
	writer.add_scalar('Loss/train', loss, epoch) # 이름, 값, epoch 횟수
	writer.add_scalar('Accuracy', loss, epoch) # 이름, 값, epoch 횟수	

writer.flush() # 값 disk에 쓰기

# path에 있는 것 GUI로 오픈해줌
%load_ext tensorboard
%tensorboard --logdir {PATH} 

```


weight & biases
- ML 실험을 위한 상용 도구. MLOps 툴로 확대중
- 프로젝트 형태로 나와서 공유하기에 좋음.
- API key 발급받아야 함. 튜토리얼 확인해보기.


### 8장
- Node (컴퓨터), GPU (컴퓨터 안의 부속 병렬 연산장치)
- NVIDIA는 TensorRT 등의 도구로 GPU 연산 


Model Parallel 모델 병렬화
- 모델의 병목이나 파이프라인으로 구현하기 어려움

```python
# to에서 어디로 보낼지 결정. 파이프라인을 치밀하게 짜야 함.
self.seq1 = nn.Sequential(self.conv1, self.bn1, self.relu, self.maxpool, self.layer1).to("cuda:1")
self.seq2 = nn.Sequential(self.layer2, self.layer3, self.avgpool).to("cuda:2") 
```


Data Parallel 데이터 병렬화
- 데이터를 나눠서 GPU에 할당하고, 결과의 평균을 취하는 방법.
- minibatch와 유사 
- Global interpreter lock과 GPU 사용 불균형 문제가 발생할 수 있음.
```python
parallel_model = torch.nn.DataParallel(model) # 이 모델로만 사용해주면 데이터 병렬이 구현됨.
```


DistributedDataParallel
- cpu마다 멀티프로세싱해 개별 GPU에 할당.
- 멀티프로세싱 import해, 통신 규약 정의하고, sampler 설정, model을 DistributedDataParallel로 재정의해주면 됨


### 9장 Hyperparam tuning

모델, 데이터, Hyperparam tuning, 3가지를 조정할 수 있다.
모델은 SOTA가 이미 고정되어있다. 데이터는 많이 모으고 잘 정제할수록 좋아진다. hyperparam은 마지막으로 짜내기.
예전에는 레시피가 중요했는데, 요즘에는 빅데이터로 커버하기 때문에 0.01 짜내자고 몇 달 소요하지는 않음.
이전만큼 중요하지는 않다!
AutoML이나 Ray 등의 툴 사용 가능.

Hyperparameter
- grid search : 하이퍼파라미터를 등간격으로 조절
- random search : 랜덤하게 조절. 오히려 잘 될 때도 있음.
- 요즘은 Bayesian optimization을 많이 함 (BOHB, 2018.)

Ray
- multi-node, multi-processing 지원 모듈. 
- ML의 병렬처리를 위해 개발된 모듈이고 de facto 표준.
- config 안에 hyperparam의 search range를 지정. 
- 스케줄러의 알고리즘을 지정해주면 효율적으로 병렬처리할 수 있음.
- Documentation 읽어보기.


### 10장
TroubleShooting

GPU 터질 때 해결, 모니터링
- out of memory, 왜, 어디서 발생했는지 추적하기 어려움
- Batch size 줄이기 ->/  GPU clean -> RUN
- nvidia-smi로 상태를 보거나, colab에서 할 떄는 GPUtil을 사용하면서 iter마다 memory leak 확인해보기

```python
!pip install GPUtil
import GPUtil
GPUtil.showUtilization()
```

- torch.cuda.empty_cache() 써보기 <-- 나는 이게 먹혔음

변수 관리
- 1d tensor (scaler) 값은 python 기본 객체로 처리 : 변수에 .item 붙이면 기본 객체로 처리됨
- 필요가 없어진 변수는 del로 memory free() 하기. (loop가 끝나도 scope 바깥에 남아있음.)

no_grad()
- infer 시점에서는 torch.no_grad()를 사용해 backward로 쌓이는 메모리가 없음.
```python
with torch.no_grad() : # GPU 덜 먹음
	for data, target in dataloader :
		output = model(data)
		loss = F.loss(output, target).item()
		...
```

Colab에서 주의사항
- 너무 큰 것은 실행하지 말 것 (CNN이나 LSTM)
- tensor의 float precision을 16bit로 양자화하는 방법도 고려

