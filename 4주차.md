

### NLP Part 1


### 1. Intro
NLP
- ACL, EMNLP, NAACL 등의 conference에서 
- 여러 문장/구절/토큰 레벨의 여러 Task들 존재. 

텍스트 마이닝
- 빅데이터 분석 (트렌드 분석 등)
- 클러스터링, 감정 분석 등

정보 검색 기술
- 추천 시스템 등.. 거의 발전이 끝남.

트랜스포머
- self-attention을 조립하는 형태 + finetuning 트랜드로 발전중.


Bag of Words 기법과 이를 활용한 Naive Bayesian Classifier
- vocab 만들기 -> one-hot으로 표현 (모든 거리는 서로 root 2. cos similarity는 0)
- Bag of word vector : one-hot의 elem-wise sum.

- BOW 벡터로 표현한 걸 4개의 클래스로 분류할 수 있다고 한다면, argmax P(class | vector) - 벡터가 주어졌을 때 각 class의 확률이 최대인 클래스를 택하게 된다.
- 해당 클래스에서 각 단어가 나타날 확률의 곱 이 최대인 클래스를 선택.

- 단 판단할 문장에 나온게 새로운 단어면 다른게 아무리 많이 나와도 0이 되어버리는 문제가 있다. 이런걸 보완하려고 regularization 등의 방법이 추가적으로 사용된다.
- 순서도 고려 몬함. 

### 2. Word Embedding

개요
- 단어를 벡터로 나타내는 방법
- 같이 나오는 단어들, 유사한 단어들은 비슷한 공간에 매칭되도록.

#### Word2Vec
- Training vector을 주변을 사용해서 
- Cat을 입력으로 주고, 주변 단어를 숨긴 채 이를 예측하도록 학습 진행.

순서
- 토큰화 -> vocab 구성 -> one-hot -> sliding window 
- 2-layer의 Autoencoder를 구성해 특정 차원으로 투사.

- ==예를 들어 `The cake is a lie`에 대해서 window size가 3이면, (cake, the)와 (cake, is) 를 구성한 후에, 원핫으로 표시해서, 오토인코더에 투사한다. 즉, cake input이 the와 is 중간쯤 어딘가에 투사되도록 학습된다.==

- 시각적으로 확인해볼 수 있음. 진짜 신기하네. 그러니까 학습 데이터를 쭉 모으면 drink|milk, drink|water, drink|juice...등등 수많은 연관 관계가 있는 벡터들끼리 엮일거고.. 그게 원핫으로 구성되어서 총 vocab이 1000개면 1000 -> 128 -> 1000의 autoencoder로 학습하고. 학습을 돌리면 돌릴수록 붙어있는 단어를 내적했을 때 1에 가까워지도록, 벡터가 유사해지도록 자동으로 학습되는거지. https://ronxin.github.io/wevi/

- 다양하게 활용됨. NLP, word similarity, translation, clustering 등등..

#### GloVe (Global Vectors for word representation)
- window 안에서 등장하는 횟수 = 두 벡터의 내적값 이 되도록.
- Word2Vec은 빈도를 간접적으로 반영했는데 (학습 횟수) 이건 직접적으로 반영하고, 중복 학습을 안 하고 그냥 loss function에 넣어서 학습을 빠르게 만들어주었음.


### 3. RNN Basic

개요
- hidden vector가 매 step마다 계속 생성된다. 중요한 건 matrix가 아니라 vector다! 
> ht = f(ht-1, xt)
- 따라서 ht-1 이 2차원, xt가 3차원이면, 이 둘을 합쳐서 ht (2차원) 으로 변환해줄 각각의 weight matrix가 필요.
- 설명 진짜 잘하신다..

RNN set
- one to many : image descriptoin 생성
- many to one : 감정 분석, 영상 분류
- many to many  : 문장 번역, 영상 분석

Char-Level language model
- 예를 들어 hello를 예측하도록 만들면, h/e/l/o로 vocab을 구축해서 one-hot으로 넘겨줌. dim=4.
- hidden은 원하는대로 만들어서.. 다음으로 계속 넘김.
- h가 들어오면 e가 예측되어야 하고, e가 들어오면 l이.. 이런식으로 다음 char를 예측하도록.
- output dim=4에 softmax를 해서 다음 나올 것이 1이 되는 label로 역전파.
- RNN의 특징 : 둘 다 l이 들어왔을 때 앞에서는 l이 나와야 하지만, 뒤에서는 o가 나와야 한다는 점. time에 따라.
- 이 역할을 hidden vector가 수행하고 있는 것
- infer 수행 시 맨 처음만 주고, output을 다음 입력으로 계속 재투여.

RNN의 역전파 (BPTT; backpropagation through time)
- 모든 time step에서의 loss를 모두 합쳐서, 모든 timestep에 분배해 전달.

XAI
- hidden vector에 정보가 담겨있다. 특정 dim에서 보면 여러 신기한 현상을 볼 수 있음 (화자 구분 등)

한계
- backpropagation의 누적으로 같은 수가 계속 곱해져서 gradient vanishinig 또는 exploding.


### 4. LSTM, GRU

#### LSTM
- vanila RNN은 hidden vector만 있었고, ht = f(ht-1, xt) 였다.
- cell state, hidden vector 두가지의 상태 벡터가 전달된다. 따라서 (ct, ht) = f(ht-1, ct-1, xt)

- 먼저 2가지를 받아 선형변환하고, 그 output을 4가지 (input, forget, output, gate) 에다 전달한다. 
- input, forget, output은 sigmoid를 통과해서, 0~1사이의 값 (정도) 을 가진다. 얼마나 보존할지.
- 그래서 gate 를 통과한 값들이 얼마나 남길지 또는 다음 cell state를 결정.
- hidden state의 일부 

#### GRU
- LSTM의 경량화. hidden vector만 가지고 있음. (cell state와 역할이 유사)
- input gate와 forget gate가 서로 반대 확률을 가지도록 작동 -> 따라서 ht의 이전과 현재의 가중 평균.

#### Backpropagation
- Whh를 계속 곱해주는 게 아니라, 
- 필요로 하는 정보를 덧셈 연산 (gradient를 복사하는 효과) -> gradient를 큰 변형 없이 long-term으로 전달 가능.


### 5. Seq2Seq - Attention

Seq2Seq
- 2개의 RNN module을 사용해 encoder, decoder.
- hidden vector에 정보가 과하게 압축되고, long-term에 약하다는 단점. 

Attention
- 중간의 모든 hidden vector를, decoder의 input과 dot product로 곱한 후 softmax
- ==즉 현재 인풋과 문장 중간 어느 상태들의 유사도가 된다.==
- 이 유사도를 기반으로 모든 encoder hidden vector의 가중평균을 구함 -> 어느 hidden vector에 집중할지 결정.
- 이것이  context vector, 모든 encoder hidden vector의 가중 평균
- context vector와 현재 decoder hidden vector를 concat 해 decoder에 투여.
- 역전파 시에도 attention score별로 gradient를 분배.

- training 시에는 GT token으로 teacher forcing함. 예측할 때는 이전 출력을 입력으로 사용.

다른 Attention 방법
- generalized attention : 어텐션 계산 시 dim별로 다른 가중치를 주는 matrix를 추가
- concat attention : encoder hidden vector와 decoder hidden vector를 쌍지어서 cat한 후 (dim, 1) MLP를 만듬.

장점
- Long-range dependency 파악 가능. bottleneck problem 해결. gradient problem 해결.
- 해석 가능성.

