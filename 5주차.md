
## NLP Part 2

### 6. Beam Search & BLEU Score

개요
- 무조건 greedy decoding하면 돌아갈 방법이 없음. 
- 시퀀스별 확률곱이라고 생각했을 때 앞에서는 적지만 뒤에서 확률이 크게 나와 보상될 수 있다.

Beam Search
- beam size k를 결정해서, 상위 몇 개의 가능성을 트리 형태로 검토한다.
- 트리 형태로 joint probability (log화해서 합)을 계산해서 최대인 경우를 고름.
- Greedy의 경우 END token이 나오면 종료되지만, 다른 시점에서 END가 나오게 됨 (그래서 문장 길이도 달라짐)
- 정해진 몇 개의 상위 문장이 나오면, 가장 큰 스코어를 고름 (문장 길이가 다르므로 길이에 따라 normalize해줌)

BLEU Score
- 문장의 순서가 중요하기보다 토큰이 몇개 겹치는지가 의미상으로 중요할 수 있다.
- precision = 예측 문장 중에서 겹치는 토큰이 몇 개인가?
- recall = ref 문장에서 겹치는 토큰이 몇 개인가? (원래 중에서 몇 개나 잘 아비터 리콜했는가 ㅋㅋㅋ)
- F-score = precision과 recall의 조화평균
- 그런데 순서도 사실 중요하잖아. reordering만 한다고 정확도가 100%인 게 아니지.

- 그래서 BLEU score (Bilingual Evaluation Understudy)는 N-gram overlap을 계산. precision만을 계산.
- 몇 개의 토큰이 동시에 겹치는가. 
- 1,2,3,4-gram에 대해서 모두 precision을 구한 후 기하평균.
- brevity penalty : GT 문장보다 짧았을 때는 precision을 낮춰주는 factor.

- 따라서 순서도 어느 정도 고려되고, 겹치는 토큰도 고려된다.

### 7-8. Transformer

Self-Attention의 작동 원리
- 한 토큰의 vector는 query, key, value로 들어가서, key와 곱해진 후 softmax 후에 -> attention score
- value들의 가중평균이 곧 output이 된다.
- 왜 Q,K,V로 분리했는가? 만일 embedding을 그대로 서로 내적해서 사용하면 자기 자신만 크게 나올 것이기 때문 -> 특정 차원으로 넘긴 후에 학습할 수 있도록 각각 별도의 공간을 만들어줌 (by linear transform)
- RNN에서는 정보가 멀리까지 전해지기 위해서는 여러 time을 거쳐야 했는데, 여기서는 즉시 관계가 연결됨 (long-term dependency를 해결)

- 자세한 QKV 어텐션 연산 설명 주어짐, 이해가 어려울 때 다시 들어보기!

- Scaling을 하는 이유는 차원수가 각각 2라면, ab와 xy를 dot product하면 ax+by (평균은 0, 분산이 2가 됨.) 
- 이러면 전체적으로 값의 크기가 너무 커져서, sofrmax 확률분포가 한 key에만 극단적으로 몰릴 수 있음.
- 따라서 var를 다시 맞춰주면서 scale을 조금 잡아주기 위해, sd로 나눠줌 -> var를 1로 유지해줌

Multi-head
- 여러 버전의 attention을 수행 -> 모든 encoding vector를 concat하고 원래 차원으로 돌린 후 다음으로 넘김
- 실제 구현체에서는 100개의 dim을 head 개수로 쪼개서 진행. 
- self-attention은 seq length `n` 과 embedding dim이 `d` 일 때 O(n^2 * d). 복잡도는 크지만 병렬화하면 한번에 계산 (공간이 많이 듬)

Block의 구성
- Multihead -> AddNorm -> MLP -> AddNorm
- Add : ResNet 테크닉. 입력값 대비 변화만을 출력하는 것.
- Norm : 표준정규분포로 잡아줌 (affine으로 최적화된 평균과 분산을 학습함). 학습 안정화, 성능 개선에 필요. 
 > 각 embedding 안에서 (0, 1^)로 맞춤.
 > 그리고 node별로 affine tranform을 수행. (y = 2x+3 으로 변환하면 평균은 3, var는 2^ = 4가 됨. 이 a,b를 학습) 

Positional Encoding
- 위치 정보가 필요해서 특정 패턴을 추가.

Warm-up lr scheduling
- lr을 끌어올렸다가 (local minima 탈출), 특정 epoch에서 점점 줄임 (수렴)

Encoder - Decoder
- Encoder에 <I, go, home> 이 들어가고, Decoder에는 <sos, 나는, 집에, 간다> 가 들어간다.
- Encoder N개 stack해 순차적으로 변환.
- Decoder는 Masked Multi-head attention 단계로 self-attetion을 거치고 -> 다음 multi-head attention에서 Query 입력으로만 사용됨 (key, value는 encoder에서 옴)
- 최종적으로 linear transform으로 원하는 vocab size로 변환해, softmax해서 원하는 단어로 매핑
- 매핑된 GT와의 차이를 역전파.

-> 순차적으로 나오는 게 맞나? 최종 출력이?? 이해가 잘 안되는데? 하나씩 나오는게 아니잖아. 문장을 통째로 투여했는데?

Masked Self-attention
- 예측을 수행해야 하는데 decoder에서 self-attention 수행 시 뒷 문장들을 보면 안된다.
- row-wise softmax 이후에 우측 상단을 모두 날림 (query가 자기 이후의 value랑 곱해진 attention score를 날림)
- 그리고 Q * K matrix를 다시 row-wise로 normalization.


### 9. Self-supervised pre-training

Trend
- 그리디에서 벗어나지 못함.
- Transfer learning (pre-traning - finetuning)-> self-supervised learning

GPT-1
- 여러 token을 사용해 다양한 task 수행 가능

BERT
- bi-directional, Masked Language Model과 Next Sentence Prediction task로 학습.
- WordPiece (합성 요소 반영), 학습 positional embedding, <CLS>, <SEP> 토큰 사용 등의 특징
- GPT보다 전체적으로 더 weight, batch size가 큼. 

GPT-2
- 다음 단어 예측 task (uni-directional). 따라서 masked self attention을 사용하는 Decoder block만 사용

Machine Reaidng Comprehension (읽기 기반의 독해) Task
- SQuAD Dataset : 지문을 읽고 독해하는 데이터셋.
- 어느 위치에 답이 있는지를 예측하기 위해 full connected layer를 통과. 그래서 시작-끝 지점을 특정하도록 학습.
- SQuAD 2.0에서는 답이 없는 경우도 포함되어 있어서, 우선 CLS로 답이 있는지/없는지를 classification 후, 위치 특정


### 10. Advanceed self-supervised learning

GPT-2
- 좋은 Dataset 40GB 사용, zero-shot (fine-tuning 없이) setting으로 바로 task 수행 가능.
- motivation : 모든 자연어처리 task를 Question-Answer 문제로 치환할 수 있다는 배경에서 시작.
- Reddit에서 링크가 달렸으면 거기까지 들어가서 텍스트를 가져와서 질문답변을 학습함.
- BPE로 tokenization, 위쪽 layer의 가중치를 더 작게.
- 요약 task도 TLDR 토큰을 주면 1줄요약.
- Translation Task도, "they say in French" 등의 문장을 뒤에 주면 번역해줌.

GPT-3
- 더 많은 양의 param과 block, batch size.
- 모델 구조 변경 없이도 prompt에서 예시를 주기만 해도 task 수행이 가능해짐 (tuning 없이도 텍스트만으로 task 수행 가능)
- model size 키울수록 빠르게 성능이 올라가더라.

ALBERT
- BERT의 경량화 모델
- embedding을 작게 만들고, 투입할 때만 차원을 늘려주는 matrix를 둬서 연산량을 줄였음. 
- Block간의 weight를 모두 공유하도록 해도 (QKV matrix) 성능저하가 크게 없었음
- Sentence Order Prediction (NSP는 너무 쉬우니까 순서를 맞추자)

ELECTRA : token replacement로 효율러닝
- MASK를 복원하는 Generator 모델(BERT), 이를 구별해내는 Discriminator (ELECTRA) 모델을 결합.
- 실제로 masked나 replacement되었는지를 구별해냄, 성능 좋아짐 

다양한 방식의 경량화 모델
- 소형 디바이스와 제한된 자원으로도 infer 가능하도록 만듬
- Distill-BERT : distill은 큰 teacher 모델이 작은 student 모델을 모사할 수 있게 만드는 기법. 
- TinyBert : 중간 결과 벡터들 (block 사이사이)도 teacher와 가까워지도록 학습. 

최신 연구 흐름
- Knowledge Graph : 문맥을 파악하지만 추가적 정보를 추론하거나 활용하지 못함. 그래서 일반 상식 활용이 중요해짐
- ERNIE : enhanced lang representation with informative entities
- KagNET : Knowledge-aware graph networks for commonsense reasoning 

