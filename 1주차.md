
몰랐던 내용, 알고 있었지만 희미한 내용, 기억해둘만한 내용들을 기록합니다.

---
## Python

### 1장
절대 경로 / 상대 경로
- / : root dir
- ./ : now dir
- ../ : upper dir
- ~/ : home dir

파이썬의 특징
- OS-dependent / interpreter lang / OOP / dynamic typing
- 데이터 분석에 유용, 간단.

### 2장
컴퓨터의 반올림 오차
- 이진수 변환 시 무한소수가 되는 경우가 있음 (python 3.x부터는 정상 출력)

리스트
- 리스트 변수에는 리스트의 주소값이 저장 (not copy value) -> 원본을 수정하면 복사본도 함께 바뀜
- 값 복사 시 deepcopy 사용할 것

formatting
```python
print("Product : {0}, Price : {1:.3f}".format("Apple", 5.23612)) // 소수점 자리수 지정
print("Product:"{0:>10s}, "Price : "{1:10.3f}.".format("Apple", 5.243)) // 위치 맞추기
```


반복문
- for else , while else (단, break로 탈출할 경우 실행하지 않음)
```python
for i in range(10) :
	print(i)
else :
	print("EOP")

```

raw string
- string 선언 시 r을 붙이면 escape문자를 무시하고 출력함
```python
rstring = r"spider\nman"
```

Call by assignment
- immutable : call by value (int, str, tuple...)
- mutable : call by ref (list, set, dict...)

Type hint, Docstring
- 함수 생성 시 param type, return type, doc 명시해주면 유지보수성 좋아짐

Python coding convention
- flake8 module 또는 black module 사용

### 3장
tuple
- imuutable, () 사용
- 슬라이싱, 인덱싱 지원
- 값을 변하게 할 여지가 있는 실수를 미연에 방지하기 위해 사용

set
- 중복 불가, 슬라이싱 및 인덱싱 불가
- union, intersection, difference

dict
- keys(), items(), values()

collections
- ==deque== : stack과 queue 지원. O(1). 좌우에서 append, pop 가능, rotate() 가능
- ==Counter== : list type의 element들을 종류별로 세서 dict 형태로 만들어줌
- OrderedDict : 입력 순서대로의 dict 저장
- defaultdict : 기본 value를 지정 (0으로 초기화하고 싶을 때 사용)
- namedtuple : 클래스 스타일의, 튜플 형태로 구조체 저장

Pythonic
- split, join
- 간단한 리스트 만들기 alist = \[2 * i for i in range(5)]. 
- enum (i, v로 추출), ==zip (두 리스트를 병렬로 추출)== for a,b in zip(alist, blist)
- ==generator== (1줄 리스트와 같지만 괄호를 사용. 그 시점의 값만 생성해 메모리 아낄 수 있음)

가변 parameter
- 파라미터가 몇 개일지 모를 때, iteration으로 처리하도록.
- args는 튜플 형태, kwargs는 dict 형태
- def nem(a, b, \*args, \*\*kwargs)

### 4장
클래스와 객체
- 클래스명만 CamelCase. 나머지는 snake_case

특징
- inheritance : 하위 클래스에게 상속 가능
- polymorphism : method overriding
> overriding : 같은 이름, 같은 파라미터. 동일 이름의 method가 다른 클래스 (상위-하위 등) 안에서 다른 동작 하는 것.
> overloading : 같은 이름, 다른 파라미터. 동일 클래스 내에서 같은 이름의 여러 method 존재하는 것
- encapsulation : 정보 저장 및 은닉. private는 \_\_로  표시
- abstraction : 하위에서 구체적으로 생성.

Decoration
- closure : inner function을 return하는 구조. 다양한 변형형 함수를 만들 수 있다는 장점.
- decorate하면, decorated function이 decorate function의 param으로 넘어간다.  
- decorate 함수 이전/이후 또는 둘러싼 동작 실행 가능

모듈과 패키지
- module (single .py file) / package (multiple .py file)
- import random : random.~ 로 사용해야 함. random.randint(1,10)
- import random as rd : alias. rd.randint(1,10)
- from random import * : 현재 .py에 모든 코드를 복사한 것처럼 (include) 그대로 쓸 수 있음. randint(1,10)
- from random import randint : randint만 가져와서 그대로 쓸 수 있음.

### 5장

Error 종류
- IndexError (초과),
- NameError (없는 변수)
- ZeroDivisionError
- ValueError (타입)
- FileNotFoundError (파일)

try except else finally / raise / assert
- except - else : 예외 발생 시 / 예외 발생하지 않은 경우
- finally : 예외 발생과 무관하게 실행
- raise : 특정 조건에서 예외 발생시킴
- assert : False일 때 예외 발생. 

File IO
- encodings = "utf-8" 사용.
- with open(filename, mod, encodings) 사용
- 객체를 통째로 박제하고 싶을 때는 pickle 사용
- 로그 남길 때는 logging 모듈 활용 (debug, info, warning, error, critical)
- FileHandler로 로그 저장할 파일을 생성해, addHandler로 추가하면 로그가 기록됨.

설정값 parsing
- configparser : 미리 txt 등으로 저장된 설정 정보를 loading. 
- argparser : 실행 시 parameter로 넘김. add_argument로 설명과 타입 등 추가.

데이터 저장 방식
- csv : csv package로 읽기, pandas로 읽을 수도 있음. delimeter가 아닌 "," 를 전처리해야 함.
- xml : 트리구조, markup 언어, beautifulSoup4으로 parse 가능.
- json : dict 타입과 competible. json module 있고, 널리 사용됨

==정규 표현식 (regex 추가적으로 연습해두기!)==
- 문자열 패턴을 정의하는 방식
- \[] 안에 매칭되는 알파벳, 
- 메타 문자
> . : 모든 문자와 매치
> * : 앞의 글자가 반복해서 나올 수 있음
> + : 앞에 있는 글자가 1회 이상 반복
> {m, n} : 반복 횟수 지정
> ? : 반복 횟수가 1회
> | : or
> ^ : not
- re 모듈 사용 시 search()나 findall()로 탐색하여 tuple을 반환해줌. 



### 6장 numpy
np.array 
- dtype을 정의해야 함, 파이썬의 리스트와 동일, c level의 arr.
- .dtype : 데이터 형태 / .shape 으로 dim. / ndim은 rank / .nbytes

reshape()
- elem의 개수가 맞아야 함.
- size가 알아서 결정되게 하려면 -1을 사용해도 됨
- original 변수는 바뀌지 않음, 새로운 ndarray를 리턴하므로 새로 할당해주어야 함
```python
tmp = np.zeros(8)
tmp2 = tmp.reshape(2,-1) # reshape시 new ndarray가 반환됨!
```

flatten()
- ndarray method. 다차원 arr를 1차원 vector로 변환.
- 새로운 ndarray를 리턴하므로 새로 할당해주어야 함

indexing & slicing
- 쉼표로 구분된 인덱싱
- 행과 열을 나누어 slicing 가능. start, end, step으로 특정 열만 추출 가능.

생성 함수들
```python
np.arange(start, end, step).reshape(i,j) # 이렇게 생성
np.zeros(shape, dtype)
np.ones(shape, dtype)
np.empty(shape, dtype) # shape만 생성되고, init되지 않아 garbage value로 채워짐

np.zeros_like(mtx) # mtx와 동일한 shape의, 0으로 채워진 array
np.ones_like(mtx) # mtx와 동일한 shape의, 1으로 채워진 array

np.identity(n, dtype) # n * n의 I 행렬 생성.

np.random.uniform(0,1,10).reshape(2,5) # uniform dist로 0~1 사이에서 10개 추출.
np.random.normal(0,1,10).reshape(2,5) # gaussian dist로 0~1 사이에서 10개 추출.
```

axis
- 특정 dimension으로 진행되는 연산의 방식. 
- .sum(axis), .mean(axis), std(axis) ....
![[Pasted image 20231108221945.png]]


concatenate 
```python
# 웬만하면 stack으로 사용. rank가 1이어도 2로 알아서 늘려줌.
np.vstack(mtx1, mtx2) # 행을 붙임. rbind() 
np.hstack(mtx1, mtx2) # 열을 붙임. cbind()
np.concatenate(mtx1, mtx2) # axis = 0 이면 vstack, axis=1이면 hstack.
```

연산자 (operators) 
- shape이 같은 경우 : element-wise operation.
- shape이 다른 경우 : broadcasting (convolution처럼 퍼지면서 연산됨. 최소 하나의 dimension은 맞아야 함)
- 행렬곱(dot product) : mtx1.dot(mtx2) 또는 mtx1 @ mtx2
- transpose는 .T 또는 transpose()

비교
```python
np.any(mtx>5) # arr의 element가 하나라도 5를 넘으면 True
np.all(mtx>5) # arr의 element가 모두 5를 넘으면 True
```
- shape가 동일하면 비교 연산에 대해 element-wise한 boolean arr를 return.

where
```python
np.where(mtx > 0, 3, 2) # where(condition, when true, when false).
np.where(mtx > 5) # 인덱스를 반환.
```
- R에서의 which와 동일. 
- isnan(), isfinite()

argmax & argmin
- 최댓값, 최솟값의 인덱스를 return.
- axis 기반으로 탐색할 수도 있음.
- argsort() 하면 sort 순서대로의 index를 뽑아줌.

boolean을 이용한 query (R에서와 동일)
```python
a = np.arange(9)
a[a > 3]
```


### 7장 pandas

기본 개요
- row = instance = tuple
- col = attr = field = feature
- df는 전체, 하나의 column vector는 series.
- dict도 변환 가능, to_csv()
- 인덱스를 기준으로 데이터가 생성되며, data 없이 인덱스를 늘리면 NaN값으로 채워짐


불러오기
```python
pd.read_csv(filename, sep=",", header = None) # Header는 col이 첫 행에 있는지.
```

인덱싱
```python
df.loc[1] # 1번 인덱스
df["age"].iloc[1:] # age col의 1번 인덱스 이후 값들. 
```
- 인덱스 값이 순서대로 되어있지 않은 경우 (별도로 지정해준 경우)에 iloc으로 실제 물리적 순서를 기준으로 인덱싱됨.
- 보통은 row 기준으로 인덱싱되고, col은 string name으로 인덱싱.
- df.index = list() 로 row index 지정 가능.

handling
```python
del df["debt"] # col 지우기
df.drop(1) # 1번 col이 drop된 df를 return함
df.drop(1, inplace=True) # df에 1번 col이ㅣ drop됨
df[["account", "debt", "name"]] # 여러 col로 selecton
```

operation
- + : index 기준으로 연산. 매칭되지 않는 경우 NaN값으로 채워짐.
- df1.add(df2, fill_value=0) : 결측값은 0으로 채움

replace
```python
df.sex.replace({"male":0, "female":1}, inplace=True) # df의 sex col을 변환해줌.
```

apply
- col 별로 특정 함수를 적용하고 싶을 때. df.apply(mean) 하면 col별 평균치가 나옴.
- applymap을 쓰면 모든 값에 일괄 적용.

pandas bulit-in function
- df.describe() : R에서 summary()
- df.race.unique() : 고유한 값들만 보여줌. R에서 unique()
- df.sort_values() : 특정 col을 기준으로 정렬
- df.age.corr(df.earn) :  `earn`과 `age` col의 상관관계. cov(공분산)도 있음.
- df.corr() : 모든 col간 상관관계.

Groupby
- split (같은 종류끼리 분리) -> apply (특정 연산 수행) -> combine (합침)
- ex. 소속 팀별 point의 합 구하기.
```python
df.groupby("Team")["Points"].sum() # team 기준으로, point의 sum을 구함.
```
- groupby를 여러번 해서 index가 여러 개 생긴 경우, unstack()을 수행하면 table 형태로 데이터를 다시 풀어줌.
- 인덱스의 레벨을 swaplevel로 변경할 수도 있음.
- axis 설정하듯이 level을 param으로 지정해줌으로써 특정 연산 (std, sum) 등을 수행 가능

Crosstab
- A가 B와 어떻게 연관되어있는지를 표시하는 표. (graph와도 비슷?)

Merge
```python
pd.merge(df1, df2, on="id") # id를 기준으로 병합. 
pd.merge(df1, df2, left_on="id1", right_on="id2")
pd.merge(df1, df2, on="id", how = "outer") # how로 join 방식 지정 가능. default는 inner join.
```
- join의 4가지 개념 : inner join (공통만), full join (합집합), left join (왼쪽 기준으로 병합), right join (오른쪽 기준으로)

Concat
```python
df = pd.concat([df1, df2])
df1.append(df2)
```

persistence
- sqlite3로 db와 연결 가능.
- openpyxl 등을 사용해 xlsx과도 연결 가능.

---
## AI MATH

### 1장
#### 벡터
- n개의 축을 가지는 공간에서의 한 점
- 원점으로부터의 위치(이자 방향)
- 벡터 * 스칼라 = 길이만 변함
- 두 벡터의 합과 차는 상대적 위치이동을 표현.

#### Norm
- 원점으로부터의 거리. 
- L1 Norm일 경우 원점과의 manhattan distance (성분의 절댓값들의 합)
- L2 Norm일 경우 원점과의 euclidean distance

#### 내적
$$<x,y> = |x|_2 \times |y|_2 \times cosθ$$
- 따라서 cos는
$$ cosθ = {<x,y> \over |x|_2 * |y|_2}$$
- 따라서 만일 내적값이 0이면 cosθ = 0, 따라서 두 벡터는 perpendicular한 관계.
- 내적값은 두 벡터의 cos 유사도. 
- numpy에서는 inner와 l2_norm을 이용해 각도를 구할 수 있음.
```python
def angle(x,y) :
	v = np.inner(x,y) / (l2_norm(x) * l2_norm(y))
	theta = np.arccos(v) # 역코사인으로 각도 구함
	return theta
```


### 2장
#### 행렬
- 벡터를 원소로 가지는 2차원 배열.
- 원소는 x_ij로 표현
- transpose 시 i,j 인덱스가 바뀜 (diagonal 축을 기준으로 flip)
- 공간에서의 여러 점 (행벡터, 또는 열벡터들) 의 모임으로 생각할 수 있고
- 또는 특정 차원으로의 선형변환으로 해석할 수 있음 -> 패턴 추출, dimension reduction 등

#### 행렬곱
- i 행벡터와 j 열벡터의 내적. 
- numpy의 operator는 @
- np.inner (내적)와 다름
- AB와 BA는 다름.
- A가 (m, n)이고 B가 (n, k) 로 숫자가 맞아야 (m, k) 형태의 행렬곱을 얻을 수 있음.

#### 역행렬
- A의 연산을 되돌리는 행렬로, 어느 방향에서 곱해도 I가 나와야 함 $A A^{-1} = A^{-1} A= I$ 
- n * n 행렬에서만 계산 가능.
- determinant가 0이 아닌 경우에만 계산 가능 (차원이 squash되지 않은 경우만)
```python
X = np.arange(9).reshape(3,3)
Xinv = np.linalg.inv(X) # linalg는 linear algebra의 약자.
```

#### 유사역행렬
- (정사각행렬이 아니라서) 역행렬을 계산할 수 없다면, pseudo-inverse (Moore-Penrose) 역행렬을 이용.
- 억지로 전치행렬을 곱해줘서 정사각행렬로 만든 후에 I가 되는 걸 찾는 방식.
- $A^+$로 표현하며, 행과 열, 어느 쪽이 더 많은지에 따라 연산 방법이 달라짐. ==추가 공부 필요!==

![[Pasted image 20231109105742.png]]

#### 응용
- 연립방정식 풀기
- L2 Norm을 최소화하는 선형회귀식 구하기


### 3장 & 4장
경사하강법
- 미분값의 반대방향으로 이동함으로써 local mininum에 도달
- `lr` :  한 step에 얼마나 이동할지
- `eps` : 학습 종료 조건을 만족하는 최소 차이 (이정도면 극소값 도달 판정)
- 각 변수별로 편미분을 계산한 gradient vector를 계산해두고 경사하강법에 이용 가능

선형회귀분석 by 경사하강법
- 기본적으로 erorr fucntion과 gradient vector를 알아야 함. 
- Error function은 pred Y와 observed Y의 차이값의 L2 Norm.
- 이를 각 계수(ax+b일 경우 a,b)에 대해 각각 편미분한 값들의 합.
- 학습 종료 조건은 알아서 설정.
- ==gradient vector의 중요 요소는 (앞에 계수들은 lr로 뺐다고 치고) Xt와 y - Xb의 곱.==
- lr과 iteration, init value가 중요한 hyper param.
- 선형회귀의 경우 global minimum 수렴이 보장되지만, 비선형에선 그렇지 않음.

Stochastic Gradient Descent
- 데이터 일부를 사용해 업데이터함. (mini batch 활용 시, mini batch-SGD)
- batch를 사용하는 경우 기댓값은 비슷하면서 연산량은 줄일 수 있음
- 극소점에서의 탈출이 가능하다는 장점도 있음 

### 5장
Neural Network
- 선형 함수 + 비선형 함수의 결합 (multilayered perceptron)
- 하나의 층은 행렬식으로 표현됨
- y = w1x1 + w2x2 + w3x3 + ... wnxn + b
- input -> w -> activation function
- 현재는 ReLU를 많이 사용 (미분 간단, vanishing gradient 문제 해결)

MLP multilayer perceptron
- 2층 신경망으로도 연속함수 근사가 가능하지만, 층이 깊을수록 node 숫자가 줄어듦.

역전파 알고리즘
- gradient vector를 delta rule로 역순으로 계산.

### 6장
확률분포
- 데이터가 존재할 수 있는 경우에 따른 확률.
- discrete : PMF (mass)
- continuous : PDF (density)
- 조건부 확률 : 특정 Y 조건 하에서 (sample space가 변함)

기댓값, 분산, cov
- E(x) = $\sum_{i=1}^{n} x*P(x)$
- V(x) = E((x - m)^2) = E(x^) - E(x)^
- Cov(x1, x2) = E\[(x1-m1)(x2-m2)]

몬테 카를로 샘플링
- 확률 분포를 모를 때 랜덤 샘플링으로 기대값을 대신 계산 가능.
- 확률 분포를 알더라도 적분할 수 없는 경우 몬테 카를로를 사용해야 함.
- 원하는 적분값을 알고싶을 때 주변에 사각형을 친 후, 점을 추출해 비율을 계산해서, 그 비율에 전체 넓이를 곱해줌.

### 7장
통계 모델링
- 적절한 확률분포를 선정해 (원리를 고려) 모수를 추정 -> 검정
- 표본평균의 분포는 횟수 N이 커질수록 (분포에 상관없이) 정규분포를 따름. (Central Limit Theorem)

최대 가능도 추정 (Max Liklihood Estimation)
- 가장 가능성이 높은 모수를 추정하는 방법
- argmax L(θ; x) = argmax P(x|θ)
- log화하면 곱셈을 덧셈으로 바꿀 수 있음. (컴퓨터에서 용이한 계산을 위해)
- 손실함수로 설정하는 경우, negative log-liklihood 를 통해 최적화.

정규분포일 경우
- mean과 sd를 기준으로 log liklihood를 미분하면,
- 최적화된 모수 mean은 산술 평균, var는 E((x-m)^) 이 나옴 (정의대로)

딥러닝에서의 최대 가능도 
![[Pasted image 20231110013835.png]]

손실함수의 본질
- 두 확률분포의 거리를 최소화
- 확률분포의 거리는 KL divergence
![[Pasted image 20231110013923.png]]
- Cross entropy = Entropy + KL divergence
- pred(Q)와 label (P) 간 거리 (KL divergence)를 최소화하는 문제와 같아짐.


### 8장
조건부 확률
- P(A|B) = P(ANB) / P(B). 
- B가 일어난 후 (그 상태에서) A가 일어날 확률은 sample space를 B로 국한하는 것과 같다.
- False Positive (alpha error) / False Negative (beta error)
- 이에 따라 specificity (False를 얼마나 잘 거르나), recall (True를 얼마나 잘 맞추는지) 등의 metric을 얻을 수 있음.

베이즈 통계학
- 새로운 데이터가 들어왔을 때, 사후 확률을 사전확률로 사용해 갱신된 사후 확률 계산 가능.
- 인과 관계를 모두 판단할 수 없음. confounding factor 중첩 효과를 제거해야 spurious correlation을 제거 가능.

### 9장 CNN
Convolution 연산
- 커널 안에서 움직이는 합성 함수.
- 파라미터의 개수를 커널의 개수로만 가져도 연산 가능.
- 수학적으로는, 입력 x에 대해, 두 함수 f와 g를, z를 움직여가면서 적분하는 형태.
- 영상 처리에서 conv 연산은 커널의 종류에 따라 noise 제거, 선 추출 등에 사용해왔음.

2차원 Conv
- 입력이 (H,W), 커널 크기가 (h, w), 출력 크기가 (Oh, Ow), padding이 P, stride가 S면,
- Oh = ( (H - h + 2P) // s) + 1
- Ow = ( (W - w + 2P) // s) + 1

3차원 Conv (tensor)
- 채널이 RGB인 경우, 3개의 kernal과 각각의 채널을 연산하여 모두 합함 -> 2차원이 됨
- 출력을 3차원 tensor로 만돌고 싶은 경우, kernal의 개수를 채널 개수 * n개 로 만들면 n겹의 이미지 출력이 나옴.

Convolution의 역전파
- 역전파에서도 conv 연산이 나오게 됨
- 각 output 위치에서 나오게 된 커널값과의 곱이 입력층으로 전달
- 커널 weight 입장에서는 일반적인 역 conv 연산과 동일해짐.


Pooling  (특정 값만 꺼내기)
- max pooling : 특정 커널 필터에서 max값만 고르는 작업 (resolution을 줄이는 작업)
- noise 제거, 계산 복잡도 제거


### 10장 RNN
시퀀스 데이터
- iid 가정을 위배. 조건부 확률의 파이곱으로 표현 -> 이전까지의 시퀀스 정보로 현재를 예측
- 길이가 가변적인 데이터를 다룰 수 있는 모델 필요

RNN
- 이전까지의 값들에 대한 latent variable (H)을 만들어 다음 정보를 예측
- wx + b에다가 wh_t-1 항을 더함.
- 역전파시 과거까지 graident를 전파함 (BPTT) -> gradient가 소실되기 쉽고, 메모리 자원 과도 소모
- 그래서 GRU나 LSTM으로 memory 개념을 도입


